<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"/><meta name="theme-color" content="#222"/><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"/><meta name="renderer" content="webkit"/><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"/><link rel="alternate" href="/rss.xml" title="木语" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="木语" type="application/atom+xml"><link rel="alternate" type="application/json" title="木语" href="https://koen666.github.io/feed.json"/><link rel="preconnect" href="https://s4.zstatic.net"/><link rel="preconnect" href="https://at.alicdn.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="modulepreload" href="/js/siteInit.js"></link><link rel="modulepreload" href="/js/nyx-player-RT6Y6A2P.js"></link><link rel="modulepreload" href="/js/copy-tex-XZBHQKN2.js"></link><link rel="modulepreload" href="/js/post-2QNEWI46.js"></link><link rel="modulepreload" href="/js/chunk-KZR3QQFA.js"></link><link rel="modulepreload" href="/js/index.esm-JYVAQ62Y.js"></link><link rel="modulepreload" href="/js/chunk-RK7HQRIO.js"></link><link rel="modulepreload" href="/js/chunk-FQAC5HAL.js"></link><link rel="stylesheet" href="/css/siteInit.css" media="none" onload="this.media&#x3D;&#39;all&#39;"></link><link rel="preload" href="/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E4%BA%8C%E6%AC%A1%E5%85%83%E7%BE%8E%E5%A5%B3-%E5%8A%A8%E6%BC%AB.png" as="image" fetchpriority="high"><link rel="preload" href="/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%8A%A8%E6%BC%AB-%E7%BE%8E%E5%A5%B3.png" as="image" fetchpriority="high"><link rel="preload" href="/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%8A%A8%E6%BC%AB-%E7%BE%8E%E5%A5%B3.png" as="image" fetchpriority="high"><link rel="preload" href="/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%8A%A8%E6%BC%AB%E5%A5%B3%E5%AD%A9-%E6%B5%B7%E6%B4%8B%E5%A5%B3%E5%AD%A9.png" as="image" fetchpriority="high"><link rel="preload" href="/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%88%9D%E9%9F%B3%E6%9C%AA%E6%9D%A5-%E5%8A%A8%E6%BC%AB.png" as="image" fetchpriority="high"><link rel="preload" href="/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%88%9D%E9%9F%B3%E6%9C%AA%E6%9D%A5-%E5%8A%A8%E6%BC%AB.png" as="image" fetchpriority="high"><meta name="keywords" content="theory,"/><meta name="description" content="本文从最基础的贝叶斯概型开始介绍，跨越朴素贝叶斯与概率图模型，最终深入探讨贝叶斯神经网络（BNN）的核心原理与不确定性建模。"/><link rel="canonical" href="https://koen666.github.io/2026/01/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B/"><link rel="stylesheet" href="/css/post.css?v=0.5.4"><link rel="stylesheet" href="/css/mermaid.css?v=0.5.4"><!-- 临时处理--><link rel="stylesheet" media="none" onload="this.media='all'" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css"><title>一文讲通贝叶斯理论：从概率公式到贝叶斯神经网络</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="pagefind_mount"></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">一文讲通贝叶斯理论：从概率公式到贝叶斯神经网络</h1><div class="meta"><span class="item" title="Created: 2026-01-29 00:00:00"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">Posted on</span><time itemprop="dateCreated datePublished" datetime="2026-01-29T00:00:00+08:00">2026-01-29</time></span><span class="item" title="Symbols count in article"><span class="icon"><i class="ic i-pen"></i></span><span class="text">Symbols count in article</span><span>3.4k</span><span class="text">words</span></span><span class="item" title="Reading time"><span class="icon"><i class="ic i-clock"></i></span><span class="text">Reading time</span><span>3 mins.</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">木语</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><ul><li class="item" style="background-image: url(&quot;/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E4%BA%8C%E6%AC%A1%E5%85%83%E7%BE%8E%E5%A5%B3-%E5%8A%A8%E6%BC%AB.png&quot;);"></li><li class="item" style="background-image: url(&quot;/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%8A%A8%E6%BC%AB-%E7%BE%8E%E5%A5%B3.png&quot;);"></li><li class="item" style="background-image: url(&quot;/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%8A%A8%E6%BC%AB-%E7%BE%8E%E5%A5%B3.png&quot;);"></li><li class="item" style="background-image: url(&quot;/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%8A%A8%E6%BC%AB%E5%A5%B3%E5%AD%A9-%E6%B5%B7%E6%B4%8B%E5%A5%B3%E5%AD%A9.png&quot;);"></li><li class="item" style="background-image: url(&quot;/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%88%9D%E9%9F%B3%E6%9C%AA%E6%9D%A5-%E5%8A%A8%E6%BC%AB.png&quot;);"></li><li class="item" style="background-image: url(&quot;/images/bg/%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%88%9D%E9%9F%B3%E6%9C%AA%E6%9D%A5-%E5%8A%A8%E6%BC%AB.png&quot;);"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemListElement itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope="itemscope" itemtype="https://schema.org/ListItem"><a href="/categories/machinelearning/" itemprop="item" rel="index" title="Inmachinelearning"><span itemprop="name">machinelearning<meta itemprop="position" content="0"/></span></a></span></div><article class="post block" itemscope="itemscope" itemtype="http://schema.org/Article" data-pagefind-body="data-pagefind-body" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://koen666.github.io/2026/01/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B/"/><span hidden="hidden" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/head.png"/><meta itemprop="name" content="koen"/><meta itemprop="description" content="技术是逻辑，木是自然，语是表达, Spark!"/></span><span hidden="hidden" itemprop="publisher" itemscope="itemscope" itemtype="http://schema.org/Organization"><meta itemprop="name" content="木语"/></span><div class="body md" itemprop="articleBody"><p>在这个由数据驱动的时代，传统的确定性算法习惯告诉我们“是”或“否”。但在真实世界，尤其是复杂的物理系统或灾害推演中，充满了<strong>不确定性</strong>。</p>
<p><strong>贝叶斯理论（Bayesian Theory）</strong> 提供了一种从不确定性中寻找真理的数学框架。它不只是一组公式，更是一种思维方式：<strong>根据新的证据，不断更新我们对世界的认知。</strong></p>
<p>本文将带你走过一条从经典统计学通往现代深度学习的演进之路：</p>
<ol>
<li><strong>贝叶斯定理</strong>：一切的起源。</li>
<li><strong>朴素贝叶斯</strong>：为了计算效率的“独立性妥协”。</li>
<li><strong>贝叶斯网络 (PGM)</strong>：描述万物因果的复杂图谱。</li>
<li><strong>贝叶斯神经网络 (BNN)</strong>：给深度学习装上“懂得怀疑”的大脑。</li>
</ol>
<hr />
<h2 id="1-一切的起点贝叶斯定理"><a class="anchor" href="#1-一切的起点贝叶斯定理">#</a> 1. 一切的起点：贝叶斯定理</h2>
<p>如果你只能记住一个概率公式，那必须是贝叶斯公式。它的核心逻辑是：<strong>后验概率 = (似然性 × 先验概率) / 标准化常数</strong>。</p>
<h3 id="11-数学形式与物理含义"><a class="anchor" href="#11-数学形式与物理含义">#</a> 1.1 数学形式与物理含义</h3>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>H</mi><mi mathvariant="normal">∣</mi><mi>E</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>E</mi><mi mathvariant="normal">∣</mi><mi>H</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>H</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>E</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span> (Hypothesis，假设)</strong>：我们想要验证的结论（例如：发生了山洪）。</li>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span> (Evidence，证据)</strong>：我们观测到的事实（例如：降雨量超过 100mm）。</li>
</ul>
<p>这个公式告诉我们如何通过观测来修正信念：</p>
<ul>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>H</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(H)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mclose">)</span></span></span></span> 先验概率</strong>：在拿到数据之前，根据历史经验，山洪发生的概率是多少？</li>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>E</mi><mi mathvariant="normal">∣</mi><mi>H</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(E|H)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mclose">)</span></span></span></span> 似然概率</strong>：如果真的发生了山洪（假设成立），那么出现“降雨量&gt;100mm”这种现象的概率有多大？</li>
<li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>H</mi><mi mathvariant="normal">∣</mi><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(H|E)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mclose">)</span></span></span></span> 后验概率</strong>：<strong>在观测到暴雨后</strong>，我们要把“发生山洪”的概率修正到多少？</li>
</ul>
<h3 id="12-直观理解逆向推断"><a class="anchor" href="#12-直观理解逆向推断">#</a> 1.2 直观理解：逆向推断</h3>
<p>贝叶斯定理之所以强大，是因为它允许我们进行<strong>逆向概率推断</strong>。</p>
<p>通常我们容易获得“正向数据”（比如统计过去所有山洪事件中，有多少次是暴雨引发的）。但我们在应用中往往面临的是反向问题：现在正在下暴雨，未来发生山洪的几率有多大？贝叶斯公式就是连接这两者的桥梁。</p>
<hr />
<h2 id="2-工程的妥协朴素贝叶斯-naive-bayes"><a class="anchor" href="#2-工程的妥协朴素贝叶斯-naive-bayes">#</a> 2. 工程的妥协：朴素贝叶斯 (Naive Bayes)</h2>
<p>当我们需要处理的特征非常多时（例如：判断灾害需要考虑降雨、坡度、植被、土壤湿度等几十个维度），计算所有特征的<strong>联合分布</strong>会变得异常困难。</p>
<p>为了让计算变得可行，数学家做了一个“天真（Naive）”的假设。</p>
<h3 id="21-朴素在哪里"><a class="anchor" href="#21-朴素在哪里">#</a> 2.1 “朴素”在哪里？</h3>
<p>朴素贝叶斯假设：<strong>所有的特征 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, ..., x_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 之间是相互独立的</strong>。</p>
<p>这意味着，它认为“降雨量”和“土壤湿度”之间没有关系（虽然这在物理学上显然是错的）。但这个假设将复杂的联合概率简化为了简单的连乘：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>∝</mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(y|x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i|y)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p>
<h3 id="22-为什么它依然有效"><a class="anchor" href="#22-为什么它依然有效">#</a> 2.2 为什么它依然有效？</h3>
<p>尽管独立性假设不符合物理直觉，但在高维分类任务（如垃圾邮件识别、简单的文本分类）中，朴素贝叶斯依然表现出色。这是因为我们主要关注的是<strong>哪个类别的概率最大</strong>，而不是精确的概率值。只要各特征对分类的“投票方向”一致，独立性假设带来的偏差就不会改变最终的分类结果。</p>
<hr />
<h2 id="3-结构化的因果概率图模型-pgm-贝叶斯网络"><a class="anchor" href="#3-结构化的因果概率图模型-pgm-贝叶斯网络">#</a> 3. 结构化的因果：概率图模型 (PGM) / 贝叶斯网络</h2>
<p>但是在很多场景中，不能容忍“特征独立”这种天真的假设。我们需要描述<strong>因果链条</strong>。</p>
<h3 id="31-经典的洒水车-下雨模型"><a class="anchor" href="#31-经典的洒水车-下雨模型">#</a> 3.1 经典的“洒水车-下雨”模型</h3>
<p><img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/SimpleBayesNet.svg/600px-SimpleBayesNet.svg.png" alt="简单的贝叶斯网络DAG" /></p>
<p><em>(图源：Wikimedia Commons. 这是一个经典的有向无环图 (DAG)。草湿了 (Grass Wet) 既可能是因为下雨 (Rain)，也可能是因为洒水车 (Sprinkler))</em></p>
<h3 id="32-灾害链中的应用"><a class="anchor" href="#32-灾害链中的应用">#</a> 3.2 灾害链中的应用</h3>
<p>给出一个博主正在做的灾害链推演模型，这个图的结构可能变成：<br />
<strong>[暴雨] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> [山洪] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> [泥石流]</strong></p>
<p>我们利用这种结构进行<strong>阈值反演</strong>：</p>
<ul>
<li><strong>正向</strong>：暴雨导致泥石流的概率是多少？</li>
<li><strong>反向</strong>：如果要求泥石流发生概率低于 1%，那么暴雨强度的<strong>警戒阈值</strong>应该设为多少？</li>
</ul>
<hr />
<h2 id="4-深度学习的进化贝叶斯神经网络-bnn"><a class="anchor" href="#4-深度学习的进化贝叶斯神经网络-bnn">#</a> 4. 深度学习的进化：贝叶斯神经网络 (BNN)</h2>
<p>传统的深度神经网络（DNN）虽然在拟合能力上远超上述模型，但它有一个致命弱点：<strong>盲目自信</strong>。</p>
<p>传统神经网络是<strong>确定性</strong>的：输入数据，经过固定的权重计算，给出一个确定的分数。即使输入一张完全无关的噪声图，模型也可能以 99% 的置信度将其分类为“猫”。这在自动驾驶或灾害预警中是极度危险的。</p>
<h3 id="41-从点到分布"><a class="anchor" href="#41-从点到分布">#</a> 4.1 从“点”到“分布”</h3>
<p><strong>贝叶斯神经网络 (BNN)</strong> 将贝叶斯概率论引入了深度学习。它的核心改变在于<strong>权重 (Weights)</strong>。</p>
<ul>
<li><strong>传统 NN</strong>：权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">w = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.5</span></span></span></span>（每个神经元连接是一个固定的数字）。</li>
<li><strong>贝叶斯 NN</strong>：权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>∼</mo><mi>N</mi><mo stretchy="false">(</mo><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w \sim N(\mu, \sigma^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>（每个神经元连接是一个<strong>高斯分布</strong>）。</li>
</ul>
<p><img loading="lazy" src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/600px-Artificial_neural_network.svg.png" alt="神经网络结构" /></p>
<p><em>(图源：Wikimedia Commons. 在 BNN 中，图中每一条连接线的权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 不再是一个固定的数字，而是一个<strong>高斯分布</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo stretchy="false">(</mo><mi>μ</mi><mo separator="true">,</mo><mi>σ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N(\mu, \sigma)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mclose">)</span></span></span></span>)</em></p>
<p>这意味着，网络本身不再是一个死板的函数，而是一个<strong>概率分布系综</strong>。</p>
<h3 id="42-两种不确定性"><a class="anchor" href="#42-两种不确定性">#</a> 4.2 两种不确定性</h3>
<p>BNN 最强大的能力在于它能区分两种“不知道”：</p>
<ol>
<li><strong>偶然不确定性 (Aleatoric Uncertainty)</strong>：源于数据本身的噪声（如传感器误差）。即便增加数据也无法消除。</li>
<li><strong>认知不确定性 (Epistemic Uncertainty)</strong>：源于模型见识太少（如训练数据未覆盖的场景）。<strong>BNN 会通过权重的剧烈波动，给出平坦的预测分布，告诉人类“我没见过这个，我不确定”。</strong></li>
</ol>
<h3 id="43-推理机制变分推断-variational-inference"><a class="anchor" href="#43-推理机制变分推断-variational-inference">#</a> 4.3 推理机制：变分推断 (Variational Inference)</h3>
<p>由于 BNN 的后验分布极其复杂，无法直接计算。在实际工程中，我们通常采用<strong>变分推断</strong>或 <strong>MC Dropout</strong> 等方法进行近似。</p>
<p>简单来说，就是在预测时，不是只跑一次网络，而是让网络在权重的分布中随机采样多次（比如 50 次），得到 50 个不同的结果。</p>
<ul>
<li>如果这 50 个结果高度一致 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> <strong>高置信度</strong>。</li>
<li>如果这 50 个结果差异巨大 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> <strong>高不确定性（需要人工介入）</strong>。</li>
</ul>
<hr />
<h2 id="5-总结"><a class="anchor" href="#5-总结">#</a> 5. 总结</h2>
<p>从最基础的<strong>贝叶斯公式</strong>，到为了计算妥协的<strong>朴素贝叶斯</strong>，再到能够刻画因果结构的<strong>贝叶斯网络</strong>，最后到融合了深度学习拟合能力的<strong>贝叶斯神经网络</strong>，这是一条从**“规则”<strong>到</strong>“学习”<strong>，再回归到</strong>“理性怀疑”**的技术演进之路。</p>
<ul>
<li><strong>朴素贝叶斯</strong>告诉我们：有时候为了效率，可以忽略细节。</li>
<li><strong>贝叶斯网络</strong>告诉我们：世界的运作是有因果结构的，掌握结构就能反演阈值。</li>
<li><strong>BNN</strong>告诉我们：真正的智能，不仅是能给出答案，更是通过概率分布，诚实地表达自己对答案有多大把握。</li>
</ul>
<p>在未来的 <strong>AI + 科学计算（AI for Science）</strong> 领域，结合了物理机理约束（PGM思维）与深度表示学习（NN思维）的<strong>融合模型</strong>，将是解决复杂系统推演的关键钥匙。</p>
<div class="tags"><a href="/tags/theory/" rel="tag"><i class="ic i-tag"></i>theory</a><a href="/tags/bayesian/" rel="tag"><i class="ic i-tag"></i>bayesian</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">Edited on </span><time title="Modified: 2026-01-29 15:57:45" itemprop="dateModified" datetime="2026-01-29T15:57:45+08:00">2026-01-29</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>Donate</button><p>Give me a cup of [coffee]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" src="/assets/wechatpay.png" alt="koen WeChat Pay"/><p>WeChat Pay</p></div><div><img loading="lazy" src="/assets/alipay.png" alt="koen Alipay"/><p>Alipay</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>koen<i class="ic i-at"><em>@</em></i>木语</li><li class="link"><strong>Post link: </strong><a href="https://koen666.github.io/2026/01/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B/" title="一文讲通贝叶斯理论：从概率公式到贝叶斯神经网络">https://koen666.github.io/2026/01/29/贝叶斯模型/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2026/01/15/udocker/" rel="prev" itemprop="url" data-background-image="&#x2F;images&#x2F;bg&#x2F;%E3%80%90%E5%93%B2%E9%A3%8E%E5%A3%81%E7%BA%B8%E3%80%91%E5%8A%A8%E6%BC%AB%E5%A5%B3%E5%AD%A9-%E5%92%8C%E6%A0%97%E8%96%B0%E5%AD%90.png" title="让你在没有 root 权限的机子上跑镜像"><span class="type">Previous Post</span><span class="category"><i class="ic i-flag"></i>docker</span><h3>让你在没有 root 权限的机子上跑镜像</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%B8%80%E5%88%87%E7%9A%84%E8%B5%B7%E7%82%B9%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text"> 1. 一切的起点：贝叶斯定理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E6%95%B0%E5%AD%A6%E5%BD%A2%E5%BC%8F%E4%B8%8E%E7%89%A9%E7%90%86%E5%90%AB%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text"> 1.1 数学形式与物理含义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E9%80%86%E5%90%91%E6%8E%A8%E6%96%AD"><span class="toc-number">1.2.</span> <span class="toc-text"> 1.2 直观理解：逆向推断</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%B7%A5%E7%A8%8B%E7%9A%84%E5%A6%A5%E5%8D%8F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-naive-bayes"><span class="toc-number">2.</span> <span class="toc-text"> 2. 工程的妥协：朴素贝叶斯 (Naive Bayes)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#21-%E6%9C%B4%E7%B4%A0%E5%9C%A8%E5%93%AA%E9%87%8C"><span class="toc-number">2.1.</span> <span class="toc-text"> 2.1 “朴素”在哪里？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#22-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%83%E4%BE%9D%E7%84%B6%E6%9C%89%E6%95%88"><span class="toc-number">2.2.</span> <span class="toc-text"> 2.2 为什么它依然有效？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84%E5%9B%A0%E6%9E%9C%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B-pgm-%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text"> 3. 结构化的因果：概率图模型 (PGM) &#x2F; 贝叶斯网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-%E7%BB%8F%E5%85%B8%E7%9A%84%E6%B4%92%E6%B0%B4%E8%BD%A6-%E4%B8%8B%E9%9B%A8%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text"> 3.1 经典的“洒水车-下雨”模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-%E7%81%BE%E5%AE%B3%E9%93%BE%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">3.2.</span> <span class="toc-text"> 3.2 灾害链中的应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BF%9B%E5%8C%96%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-bnn"><span class="toc-number">4.</span> <span class="toc-text"> 4. 深度学习的进化：贝叶斯神经网络 (BNN)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#41-%E4%BB%8E%E7%82%B9%E5%88%B0%E5%88%86%E5%B8%83"><span class="toc-number">4.1.</span> <span class="toc-text"> 4.1 从“点”到“分布”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#42-%E4%B8%A4%E7%A7%8D%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7"><span class="toc-number">4.2.</span> <span class="toc-text"> 4.2 两种不确定性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#43-%E6%8E%A8%E7%90%86%E6%9C%BA%E5%88%B6%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD-variational-inference"><span class="toc-number">4.3.</span> <span class="toc-text"> 4.3 推理机制：变分推断 (Variational Inference)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%80%BB%E7%BB%93"><span class="toc-number">5.</span> <span class="toc-text"> 5. 总结</span></a></li></ol></div><div class="related panel pjax" data-title="Related"><ul><li ><a href="/2025/11/23/GA/" rel="bookmark" title="GA遗传算法">GA遗传算法</a></li><li  class="active"><a href="/2026/01/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B/" rel="bookmark" title="一文讲通贝叶斯理论：从概率公式到贝叶斯神经网络">一文讲通贝叶斯理论：从概率公式到贝叶斯神经网络</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="koen" src="/images/head.png"/><p class="name" itemprop="name">koen</p><div class="description" itemprop="description">Spark!</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">33</span><span class="name">posts</span></a></div><div class="item categories"><a href="/categories/"><span class="count">16</span><span class="name">categories</span></a></div><div class="item tags"><a href="/tags/"><span class="count">60</span><span class="name">tags</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/yourname" class="item github" title="https:&#x2F;&#x2F;github.com&#x2F;yourname"><i class="ic i-github"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2026/01/15/udocker/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div><div id="player"></div></main><footer id="footer"><div class="inner"><div class="widgets"></div><div class="status"><div class="copyright">&copy; 2022 -<span itemprop="copyrightYear">2026</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">koen @ 木语</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="Symbols count total">201k words</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="Reading time total">3:03</span></div><div class="powered-by">Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL = {
    ispost: true,
    path: `2026/01/29/贝叶斯模型/`,
    favicon: {
        show: `(●´3｀●) Here we go again.`,
        hide: `(´Д｀) It's a disaster!`
    },
    search: {
        placeholder: "Search for Posts",
        empty: "We didn't find any results for the search: ${query}",
        stats: "${hits} results found in ${time} ms"
    },
    nocopy: "false",
    copyright: `Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.`,
    copy_tex: false,
    katex: false,
    mermaid: false,
    audio: undefined,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">Article Timeliness Alert</span><br>This is an article published {{publish}} days ago and last updated {{updated}} days ago. Some information may have changed, so please be careful to screen it.</p></div>`,
    quiz: {
        choice: `Multiple Choice`,
        multiple: `Multiple Answer`,
        true_false: `True/False`,
        essay: `Questions`,
        gap_fill: `Gap Filling`,
        mistake: `Wrong Answer`
    }
};
</script><script src="/js/siteInit.js?v=0.5.4" type="module" fetchpriority="high" defer></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>