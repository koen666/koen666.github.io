<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>木语 • Posts by &#34;gpu&#34; tag</title>
        <link>https://koen666.github.io</link>
        <description>Spark!</description>
        <language>zh-CN</language>
        <pubDate>Sun, 05 Oct 2025 00:00:00 +0800</pubDate>
        <lastBuildDate>Sun, 05 Oct 2025 00:00:00 +0800</lastBuildDate>
        <category>KL</category>
        <category>RE-ID</category>
        <category>Koa</category>
        <category>backen</category>
        <category>nodejs</category>
        <category>cmd</category>
        <category>curl</category>
        <category>learn</category>
        <category>GA</category>
        <category>python</category>
        <category>math</category>
        <category>cypher</category>
        <category>neo4j</category>
        <category>database</category>
        <category>container</category>
        <category>docker</category>
        <category>linux</category>
        <category>virtual</category>
        <category>electron</category>
        <category>web</category>
        <category>desk</category>
        <category>MLC</category>
        <category>repo</category>
        <category>git</category>
        <category>flask</category>
        <category>gradio</category>
        <category>markdown</category>
        <category>router</category>
        <category>ip</category>
        <category>MAC</category>
        <category>uv</category>
        <category>environment</category>
        <category>deeplearning</category>
        <category>pytorch</category>
        <category>Transformer</category>
        <category>scipy</category>
        <category>latex</category>
        <category>screen</category>
        <category>termux</category>
        <category>saaborn</category>
        <category>ssh</category>
        <category>images</category>
        <category>hpc</category>
        <category>tutorial</category>
        <category>sql</category>
        <category>vue</category>
        <category>yaml</category>
        <category>argparse</category>
        <category>easydict</category>
        <category>yolo</category>
        <category>dataset</category>
        <category>AnyLabeling</category>
        <category>dataloader</category>
        <category>agent</category>
        <category>diffusionmodel</category>
        <category>generative-ai</category>
        <category>gpu</category>
        <category>data</category>
        <category>code</category>
        <category>theory</category>
        <category>bayesian</category>
        <category>collections</category>
        <item>
            <guid isPermalink="true">https://koen666.github.io/2025/10/05/%E5%A4%9Agpu/</guid>
            <title>多gpu训练方式</title>
            <link>https://koen666.github.io/2025/10/05/%E5%A4%9Agpu/</link>
            <category>gpu</category>
            <category>data</category>
            <pubDate>Sun, 05 Oct 2025 00:00:00 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;一-为什么需要多-gpu&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#一-为什么需要多-gpu&#34;&gt;#&lt;/a&gt; 一、为什么需要多 GPU？&lt;/h2&gt;
&lt;p&gt;当我们训练深度学习模型（特别是大模型）时，单张 GPU 往往不够：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据太大，单卡放不下；&lt;/li&gt;
&lt;li&gt;模型太复杂，训练太慢；&lt;/li&gt;
&lt;li&gt;想更快训练完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;于是就有了「&lt;strong&gt;多 GPU 训练&lt;/strong&gt;」这个想法：&lt;br&gt;
&lt;strong&gt;让多张显卡一起干活，分担计算任务。&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;二-多-gpu-的三种主要方式&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#二-多-gpu-的三种主要方式&#34;&gt;#&lt;/a&gt; 二、多 GPU 的三种主要方式&lt;/h2&gt;
&lt;p&gt;多 GPU 的实现方式有很多，但核心分为三类&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方式&lt;/th&gt;
&lt;th&gt;中文名称&lt;/th&gt;
&lt;th&gt;思路&lt;/th&gt;
&lt;th&gt;优缺点&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1️⃣ 模型并行（Model Parallel）&lt;/td&gt;
&lt;td&gt;模型拆开&lt;/td&gt;
&lt;td&gt;把模型的不同层分给不同 GPU&lt;/td&gt;
&lt;td&gt;适合模型太大放不下一张卡；通信频繁，实现复杂&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2️⃣ 数据并行（Data Parallel）&lt;/td&gt;
&lt;td&gt;数据拆开&lt;/td&gt;
&lt;td&gt;模型每张卡都复制一份，每张卡训练一部分数据&lt;/td&gt;
&lt;td&gt;通用、最常见&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3️⃣ 分布式数据并行（Distributed Data Parallel, DDP）&lt;/td&gt;
&lt;td&gt;高效数据并行&lt;/td&gt;
&lt;td&gt;每张卡独立进程，自动同步梯度&lt;/td&gt;
&lt;td&gt;工业级方案，最快、最稳定&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;三-核心思想数据并行&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#三-核心思想数据并行&#34;&gt;#&lt;/a&gt; 三、核心思想：数据并行&lt;/h2&gt;
&lt;p&gt;多 GPU 训练中，&lt;strong&gt;数据并行&lt;/strong&gt;是最常用的方式。&lt;/p&gt;
&lt;p&gt;其思想非常简单：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;模型每张卡都有一份副本；每张卡训练不同部分的数据；最后汇总结果。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假设你有 2 张 GPU，每次 batch 有 4 张图片：&lt;/p&gt;
&lt;pre class=&#34;shiki shiki-themes vitesse-light vitesse-dark&#34; style=&#34;background-color:#ffffff;--shiki-dark-bg:#121212;color:#393a34;--shiki-dark:#dbd7caee&#34; tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-text&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span&gt;批次数据： [1, 2, 3, 4]&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;GPU0 ← [1, 2]&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;GPU1 ← [3, 4]&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;训练流程&lt;br&gt;
1️⃣ 每张 GPU 都有一份相同的模型副本。&lt;br&gt;
2️⃣ 各 GPU 前向计算 → 得到自己的 loss。&lt;br&gt;
3️⃣ 各 GPU 反向传播 → 得到自己的梯度。&lt;br&gt;
4️⃣ &lt;strong&gt;梯度同步（allreduce）&lt;/strong&gt; → 求平均。&lt;br&gt;
5️⃣ 各 GPU 同步更新参数。&lt;/p&gt;
&lt;p&gt;这样每张 GPU 的参数始终保持一致。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;四-关键概念详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#四-关键概念详解&#34;&gt;#&lt;/a&gt; 四、关键概念详解&lt;/h2&gt;
&lt;p&gt;下面是训练中会出现的一些关键术语&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;概念&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;train&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;训练集，用于更新模型参数。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;val (validation)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;验证集，用于评估模型是否过拟合，不参与训练。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;test&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;测试集，用于最终测试模型效果。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;query&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;查询集，常见于检索任务，用来搜索匹配结果。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;gallery / bounding_box&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;检索任务中的数据库部分（比如人脸库、行人库）。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;batch（批次）&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;一次喂给模型的数据量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;split_batch&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;把一个 batch 的数据拆成几份给不同 GPU。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;allreduce&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;把多 GPU 的梯度相加、平均，让模型参数保持一致。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;scatter&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;把数据分发到多个 GPU。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;gather / concat&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;把多个 GPU 的结果合并回来。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;五-三种实现方式详解&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#五-三种实现方式详解&#34;&gt;#&lt;/a&gt; 五、三种实现方式详解&lt;/h2&gt;
&lt;h3 id=&#34;1️⃣-模型并行model-parallel&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#1️⃣-模型并行model-parallel&#34;&gt;#&lt;/a&gt; 1️⃣ 模型并行（Model Parallel）&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;思想：&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;模型太大，一张卡放不下，就把不同层放到不同 GPU 上。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;示意：&lt;/p&gt;
&lt;pre class=&#34;shiki shiki-themes vitesse-light vitesse-dark&#34; style=&#34;background-color:#ffffff;--shiki-dark-bg:#121212;color:#393a34;--shiki-dark:#dbd7caee&#34; tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-text&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span&gt;GPU0: 负责模型前半部分&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;GPU1: 负责模型后半部分&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;特点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型能变得更大；&lt;/li&gt;
&lt;li&gt;但通信频繁，效率不高；&lt;/li&gt;
&lt;li&gt;实现较复杂。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;适用：GPT、BERT 这类超大模型。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2️⃣-数据并行data-paralleldp&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#2️⃣-数据并行data-paralleldp&#34;&gt;#&lt;/a&gt; 2️⃣ 数据并行（Data Parallel，DP）&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;思想：&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;每张卡都有一份完整模型副本，但处理不同部分的数据。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;PyTorch 用法：&lt;/p&gt;
&lt;pre class=&#34;shiki shiki-themes vitesse-light vitesse-dark&#34; style=&#34;background-color:#ffffff;--shiki-dark-bg:#121212;color:#393a34;--shiki-dark:#dbd7caee&#34; tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt;model &lt;/span&gt;&lt;span style=&#34;color:#999999;--shiki-dark:#666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt; torch&lt;/span&gt;&lt;span style=&#34;color:#999999;--shiki-dark:#666666&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt;nn&lt;/span&gt;&lt;span style=&#34;color:#999999;--shiki-dark:#666666&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt;DataParallel&lt;/span&gt;&lt;span style=&#34;color:#2993a3;--shiki-dark:#5eaab5&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt;model&lt;/span&gt;&lt;span style=&#34;color:#2993a3;--shiki-dark:#5eaab5&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;流程：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据被平均分配给每张 GPU（scatter）。&lt;/li&gt;
&lt;li&gt;各 GPU 前向 &amp;amp; 反向传播。&lt;/li&gt;
&lt;li&gt;主 GPU（GPU0）收集所有梯度，求平均。&lt;/li&gt;
&lt;li&gt;GPU0 更新参数，再广播回所有 GPU。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPU0 负担过重（通信瓶颈）；&lt;/li&gt;
&lt;li&gt;不能多机训练。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;适用：快速实验、小模型。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3️⃣-分布式数据并行distributed-data-parallelddp&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#3️⃣-分布式数据并行distributed-data-parallelddp&#34;&gt;#&lt;/a&gt; 3️⃣ 分布式数据并行（Distributed Data Parallel，DDP）&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;思想：&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;数据并行的改进版，每张 GPU 独立进程，自动通信同步。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;PyTorch 用法：&lt;/p&gt;
&lt;pre class=&#34;shiki shiki-themes vitesse-light vitesse-dark&#34; style=&#34;background-color:#ffffff;--shiki-dark-bg:#121212;color:#393a34;--shiki-dark:#dbd7caee&#34; tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span style=&#34;color:#1E754F;--shiki-dark:#4D9375&#34;&gt;from&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt; torch&lt;/span&gt;&lt;span style=&#34;color:#999999;--shiki-dark:#666666&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt;nn&lt;/span&gt;&lt;span style=&#34;color:#999999;--shiki-dark:#666666&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt;parallel &lt;/span&gt;&lt;span style=&#34;color:#1E754F;--shiki-dark:#4D9375&#34;&gt;import&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt; DistributedDataParallel &lt;/span&gt;&lt;span style=&#34;color:#1E754F;--shiki-dark:#4D9375&#34;&gt;as&lt;/span&gt;&lt;span style=&#34;color:#A65E2B;--shiki-dark:#C99076&#34;&gt; DDP&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt;model &lt;/span&gt;&lt;span style=&#34;color:#999999;--shiki-dark:#666666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt; DDP&lt;/span&gt;&lt;span style=&#34;color:#2993a3;--shiki-dark:#5eaab5&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#393A34;--shiki-dark:#DBD7CAEE&#34;&gt;model&lt;/span&gt;&lt;span style=&#34;color:#2993a3;--shiki-dark:#5eaab5&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;流程图示（假设2张GPU）&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;shiki shiki-themes vitesse-light vitesse-dark&#34; style=&#34;background-color:#ffffff;--shiki-dark-bg:#121212;color:#393a34;--shiki-dark:#dbd7caee&#34; tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-text&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span&gt;Step 1: 复制模型&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;GPU0: 模型A&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;GPU1: 模型A&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;Step 2: 拆分数据&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;GPU0: batch[0:2]&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;GPU1: batch[2:4]&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;Step 3: 前向+反向&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;各GPU独立算梯度&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;Step 4: allreduce同步&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;梯度平均后广播到每张卡&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&#34;line&#34;&gt;&lt;span&gt;Step 5: 更新参数（同步）&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;一句话总结：&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DDP 是 PyTorch 官方推荐的多 GPU 训练标准方案。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;六-底层机制理解scatter-allreduce-gather&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#六-底层机制理解scatter-allreduce-gather&#34;&gt;#&lt;/a&gt; 六、底层机制理解：scatter / allreduce / gather&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;th&gt;举例&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;scatter&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;拆数据&lt;/td&gt;
&lt;td&gt;把 100 张图片分成两份发到 GPU0、GPU1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;allreduce&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;同步梯度&lt;/td&gt;
&lt;td&gt;GPU0 的梯度 + GPU1 的梯度 → 平均后广播&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;gather / concat&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;合并结果&lt;/td&gt;
&lt;td&gt;把每个 GPU 的预测拼起来做整体评估&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这些操作是 &lt;strong&gt;数据并行的核心通信机制&lt;/strong&gt;。&lt;br&gt;
在 DDP 中，它们是由 PyTorch 自动完成的。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;七-在-ddp-中训练需要注意什么&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#七-在-ddp-中训练需要注意什么&#34;&gt;#&lt;/a&gt; 七、在 DDP 中训练需要注意什么&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;事项&lt;/th&gt;
&lt;th&gt;原因&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;每个 GPU 独立进程&lt;/td&gt;
&lt;td&gt;保证同步和效率&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;使用 &lt;code&gt;DistributedSampler&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;自动分配数据给不同 GPU&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;设置环境变量（rank, world_size）&lt;/td&gt;
&lt;td&gt;标识 GPU 编号和总数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;初始化通信（init_process_group）&lt;/td&gt;
&lt;td&gt;建立 NCCL 通信通道&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;每个进程保存日志、模型&lt;/td&gt;
&lt;td&gt;避免覆盖&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;八-三个阶段的总结对比表&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#八-三个阶段的总结对比表&#34;&gt;#&lt;/a&gt; 八、三个阶段的总结对比表&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;阶段&lt;/th&gt;
&lt;th&gt;干的事&lt;/th&gt;
&lt;th&gt;特点&lt;/th&gt;
&lt;th&gt;示例&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;模型并行&lt;/td&gt;
&lt;td&gt;模型拆分&lt;/td&gt;
&lt;td&gt;各 GPU 存不同层&lt;/td&gt;
&lt;td&gt;GPT、BERT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;数据并行（DP）&lt;/td&gt;
&lt;td&gt;数据拆分 + 主卡同步&lt;/td&gt;
&lt;td&gt;简单但低效&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nn.DataParallel&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;分布式数据并行（DDP）&lt;/td&gt;
&lt;td&gt;数据拆分 + 自动同步&lt;/td&gt;
&lt;td&gt;高效工业级&lt;/td&gt;
&lt;td&gt;&lt;code&gt;DistributedDataParallel&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;九-类比理解多-gpu-像工厂流水线&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#九-类比理解多-gpu-像工厂流水线&#34;&gt;#&lt;/a&gt; 九、类比理解：多 GPU 像工厂流水线&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;工人&lt;/th&gt;
&lt;th&gt;工作&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GPU0&lt;/td&gt;
&lt;td&gt;处理样本1-50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPU1&lt;/td&gt;
&lt;td&gt;处理样本51-100&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;每个工人有同样的手册（模型）；&lt;/li&gt;
&lt;li&gt;干完活后汇报经验（梯度）；&lt;/li&gt;
&lt;li&gt;经理平均总结经验（allreduce）；&lt;/li&gt;
&lt;li&gt;更新手册；&lt;/li&gt;
&lt;li&gt;下一轮所有人继续。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这，就是数据并行和 DDP 的本质。&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
