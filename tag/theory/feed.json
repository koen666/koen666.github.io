{
    "version": "https://jsonfeed.org/version/1",
    "title": "木语 • All posts by \"theory\" tag",
    "description": "Spark!",
    "home_page_url": "https://koen666.github.io",
    "items": [
        {
            "id": "https://koen666.github.io/2026/01/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B/",
            "url": "https://koen666.github.io/2026/01/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B/",
            "title": "一文讲通贝叶斯理论：从概率公式到贝叶斯神经网络",
            "date_published": "2026-01-28T16:00:00.000Z",
            "content_html": "<p>在这个由数据驱动的时代，传统的确定性算法习惯告诉我们“是”或“否”。但在真实世界，尤其是复杂的物理系统或灾害推演中，充满了<strong>不确定性</strong>。</p>\n<p><strong>贝叶斯理论（Bayesian Theory）</strong> 提供了一种从不确定性中寻找真理的数学框架。它不只是一组公式，更是一种思维方式：<strong>根据新的证据，不断更新我们对世界的认知。</strong></p>\n<p>本文将带你走过一条从经典统计学通往现代深度学习的演进之路：</p>\n<ol>\n<li><strong>贝叶斯定理</strong>：一切的起源。</li>\n<li><strong>朴素贝叶斯</strong>：为了计算效率的“独立性妥协”。</li>\n<li><strong>贝叶斯网络 (PGM)</strong>：描述万物因果的复杂图谱。</li>\n<li><strong>贝叶斯神经网络 (BNN)</strong>：给深度学习装上“懂得怀疑”的大脑。</li>\n</ol>\n<hr />\n<h2 id=\"1-一切的起点贝叶斯定理\"><a class=\"anchor\" href=\"#1-一切的起点贝叶斯定理\">#</a> 1. 一切的起点：贝叶斯定理</h2>\n<p>如果你只能记住一个概率公式，那必须是贝叶斯公式。它的核心逻辑是：<strong>后验概率 = (似然性 × 先验概率) / 标准化常数</strong>。</p>\n<h3 id=\"11-数学形式与物理含义\"><a class=\"anchor\" href=\"#11-数学形式与物理含义\">#</a> 1.1 数学形式与物理含义</h3>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>H</mi><mi mathvariant=\"normal\">∣</mi><mi>E</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>E</mi><mi mathvariant=\"normal\">∣</mi><mi>H</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></p>\n<ul>\n<li><strong><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span></span></span></span> (Hypothesis，假设)</strong>：我们想要验证的结论（例如：发生了山洪）。</li>\n<li><strong><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>E</mi></mrow><annotation encoding=\"application/x-tex\">E</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span></span></span></span> (Evidence，证据)</strong>：我们观测到的事实（例如：降雨量超过 100mm）。</li>\n</ul>\n<p>这个公式告诉我们如何通过观测来修正信念：</p>\n<ul>\n<li><strong><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(H)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose\">)</span></span></span></span> 先验概率</strong>：在拿到数据之前，根据历史经验，山洪发生的概率是多少？</li>\n<li><strong><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>E</mi><mi mathvariant=\"normal\">∣</mi><mi>H</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(E|H)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose\">)</span></span></span></span> 似然概率</strong>：如果真的发生了山洪（假设成立），那么出现“降雨量&gt;100mm”这种现象的概率有多大？</li>\n<li><strong><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>H</mi><mi mathvariant=\"normal\">∣</mi><mi>E</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(H|E)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"mclose\">)</span></span></span></span> 后验概率</strong>：<strong>在观测到暴雨后</strong>，我们要把“发生山洪”的概率修正到多少？</li>\n</ul>\n<h3 id=\"12-直观理解逆向推断\"><a class=\"anchor\" href=\"#12-直观理解逆向推断\">#</a> 1.2 直观理解：逆向推断</h3>\n<p>贝叶斯定理之所以强大，是因为它允许我们进行<strong>逆向概率推断</strong>。</p>\n<p>通常我们容易获得“正向数据”（比如统计过去所有山洪事件中，有多少次是暴雨引发的）。但我们在应用中往往面临的是反向问题：现在正在下暴雨，未来发生山洪的几率有多大？贝叶斯公式就是连接这两者的桥梁。</p>\n<hr />\n<h2 id=\"2-工程的妥协朴素贝叶斯-naive-bayes\"><a class=\"anchor\" href=\"#2-工程的妥协朴素贝叶斯-naive-bayes\">#</a> 2. 工程的妥协：朴素贝叶斯 (Naive Bayes)</h2>\n<p>当我们需要处理的特征非常多时（例如：判断灾害需要考虑降雨、坡度、植被、土壤湿度等几十个维度），计算所有特征的<strong>联合分布</strong>会变得异常困难。</p>\n<p>为了让计算变得可行，数学家做了一个“天真（Naive）”的假设。</p>\n<h3 id=\"21-朴素在哪里\"><a class=\"anchor\" href=\"#21-朴素在哪里\">#</a> 2.1 “朴素”在哪里？</h3>\n<p>朴素贝叶斯假设：<strong>所有的特征 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_1, x_2, ..., x_n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">...</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 之间是相互独立的</strong>。</p>\n<p>这意味着，它认为“降雨量”和“土壤湿度”之间没有关系（虽然这在物理学上显然是错的）。但这个假设将复杂的联合概率简化为了简单的连乘：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo><mo>∝</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(y|x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i|y)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.9291em;vertical-align:-1.2777em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6514em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∏</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<h3 id=\"22-为什么它依然有效\"><a class=\"anchor\" href=\"#22-为什么它依然有效\">#</a> 2.2 为什么它依然有效？</h3>\n<p>尽管独立性假设不符合物理直觉，但在高维分类任务（如垃圾邮件识别、简单的文本分类）中，朴素贝叶斯依然表现出色。这是因为我们主要关注的是<strong>哪个类别的概率最大</strong>，而不是精确的概率值。只要各特征对分类的“投票方向”一致，独立性假设带来的偏差就不会改变最终的分类结果。</p>\n<hr />\n<h2 id=\"3-结构化的因果概率图模型-pgm-贝叶斯网络\"><a class=\"anchor\" href=\"#3-结构化的因果概率图模型-pgm-贝叶斯网络\">#</a> 3. 结构化的因果：概率图模型 (PGM) / 贝叶斯网络</h2>\n<p>但是在很多场景中，不能容忍“特征独立”这种天真的假设。我们需要描述<strong>因果链条</strong>。</p>\n<h3 id=\"31-经典的洒水车-下雨模型\"><a class=\"anchor\" href=\"#31-经典的洒水车-下雨模型\">#</a> 3.1 经典的“洒水车-下雨”模型</h3>\n<p><img loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/SimpleBayesNet.svg/600px-SimpleBayesNet.svg.png\" alt=\"简单的贝叶斯网络DAG\" /></p>\n<p><em>(图源：Wikimedia Commons. 这是一个经典的有向无环图 (DAG)。草湿了 (Grass Wet) 既可能是因为下雨 (Rain)，也可能是因为洒水车 (Sprinkler))</em></p>\n<h3 id=\"32-灾害链中的应用\"><a class=\"anchor\" href=\"#32-灾害链中的应用\">#</a> 3.2 灾害链中的应用</h3>\n<p>给出一个博主正在做的灾害链推演模型，这个图的结构可能变成：<br />\n<strong>[暴雨] <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>→</mo></mrow><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">→</span></span></span></span> [山洪] <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>→</mo></mrow><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">→</span></span></span></span> [泥石流]</strong></p>\n<p>我们利用这种结构进行<strong>阈值反演</strong>：</p>\n<ul>\n<li><strong>正向</strong>：暴雨导致泥石流的概率是多少？</li>\n<li><strong>反向</strong>：如果要求泥石流发生概率低于 1%，那么暴雨强度的<strong>警戒阈值</strong>应该设为多少？</li>\n</ul>\n<hr />\n<h2 id=\"4-深度学习的进化贝叶斯神经网络-bnn\"><a class=\"anchor\" href=\"#4-深度学习的进化贝叶斯神经网络-bnn\">#</a> 4. 深度学习的进化：贝叶斯神经网络 (BNN)</h2>\n<p>传统的深度神经网络（DNN）虽然在拟合能力上远超上述模型，但它有一个致命弱点：<strong>盲目自信</strong>。</p>\n<p>传统神经网络是<strong>确定性</strong>的：输入数据，经过固定的权重计算，给出一个确定的分数。即使输入一张完全无关的噪声图，模型也可能以 99% 的置信度将其分类为“猫”。这在自动驾驶或灾害预警中是极度危险的。</p>\n<h3 id=\"41-从点到分布\"><a class=\"anchor\" href=\"#41-从点到分布\">#</a> 4.1 从“点”到“分布”</h3>\n<p><strong>贝叶斯神经网络 (BNN)</strong> 将贝叶斯概率论引入了深度学习。它的核心改变在于<strong>权重 (Weights)</strong>。</p>\n<ul>\n<li><strong>传统 NN</strong>：权重 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">w = 0.5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0.5</span></span></span></span>（每个神经元连接是一个固定的数字）。</li>\n<li><strong>贝叶斯 NN</strong>：权重 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mo>∼</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mi>μ</mi><mo separator=\"true\">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">w \\sim N(\\mu, \\sigma^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">μ</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>（每个神经元连接是一个<strong>高斯分布</strong>）。</li>\n</ul>\n<p><img loading=\"lazy\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/600px-Artificial_neural_network.svg.png\" alt=\"神经网络结构\" /></p>\n<p><em>(图源：Wikimedia Commons. 在 BNN 中，图中每一条连接线的权重 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span> 不再是一个固定的数字，而是一个<strong>高斯分布</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo stretchy=\"false\">(</mo><mi>μ</mi><mo separator=\"true\">,</mo><mi>σ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">N(\\mu, \\sigma)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">μ</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mclose\">)</span></span></span></span>)</em></p>\n<p>这意味着，网络本身不再是一个死板的函数，而是一个<strong>概率分布系综</strong>。</p>\n<h3 id=\"42-两种不确定性\"><a class=\"anchor\" href=\"#42-两种不确定性\">#</a> 4.2 两种不确定性</h3>\n<p>BNN 最强大的能力在于它能区分两种“不知道”：</p>\n<ol>\n<li><strong>偶然不确定性 (Aleatoric Uncertainty)</strong>：源于数据本身的噪声（如传感器误差）。即便增加数据也无法消除。</li>\n<li><strong>认知不确定性 (Epistemic Uncertainty)</strong>：源于模型见识太少（如训练数据未覆盖的场景）。<strong>BNN 会通过权重的剧烈波动，给出平坦的预测分布，告诉人类“我没见过这个，我不确定”。</strong></li>\n</ol>\n<h3 id=\"43-推理机制变分推断-variational-inference\"><a class=\"anchor\" href=\"#43-推理机制变分推断-variational-inference\">#</a> 4.3 推理机制：变分推断 (Variational Inference)</h3>\n<p>由于 BNN 的后验分布极其复杂，无法直接计算。在实际工程中，我们通常采用<strong>变分推断</strong>或 <strong>MC Dropout</strong> 等方法进行近似。</p>\n<p>简单来说，就是在预测时，不是只跑一次网络，而是让网络在权重的分布中随机采样多次（比如 50 次），得到 50 个不同的结果。</p>\n<ul>\n<li>如果这 50 个结果高度一致 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>→</mo></mrow><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">→</span></span></span></span> <strong>高置信度</strong>。</li>\n<li>如果这 50 个结果差异巨大 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>→</mo></mrow><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">→</span></span></span></span> <strong>高不确定性（需要人工介入）</strong>。</li>\n</ul>\n<hr />\n<h2 id=\"5-总结\"><a class=\"anchor\" href=\"#5-总结\">#</a> 5. 总结</h2>\n<p>从最基础的<strong>贝叶斯公式</strong>，到为了计算妥协的<strong>朴素贝叶斯</strong>，再到能够刻画因果结构的<strong>贝叶斯网络</strong>，最后到融合了深度学习拟合能力的<strong>贝叶斯神经网络</strong>，这是一条从“规则”到“学习”，再回归到“理性怀疑”**的技术演进之路。</p>\n<ul>\n<li><strong>朴素贝叶斯</strong>告诉我们：有时候为了效率，可以忽略细节。</li>\n<li><strong>贝叶斯网络</strong>告诉我们：世界的运作是有因果结构的，掌握结构就能反演阈值。</li>\n<li><strong>BNN</strong>告诉我们：真正的智能，不仅是能给出答案，更是通过概率分布，诚实地表达自己对答案有多大把握。</li>\n</ul>\n<p>在未来的 <strong>AI + 科学计算（AI for Science）</strong> 领域，结合了物理机理约束（PGM思维）与深度表示学习（NN思维）的<strong>融合模型</strong>，将是解决复杂系统推演的关键钥匙。</p>\n",
            "tags": [
                "theory",
                "bayesian"
            ]
        }
    ]
}